\documentclass{acm_proc_article-sp}

\usepackage{graphicx}


\usepackage{url}
\usepackage{hyperref} 
\hypersetup{breaklinks} 

\begin{document}
\title{LRT: The Live Remote Trace service for Mobile Debugging}
\author{Cody Hanson, Ross Nordstrom\\
        University of Colorado - Colorado Springs\\
        1420 Austin Bluffs Pkwy,\\
        Colorado Springs, CO 80918

        \texttt{\{chanson,rnordstr\}@uccs.edu}
       }
\date{July 2013}

\maketitle

\begin{abstract}
    As mobile applications become increasingly  more complex and integrated into their
    environments, testing and debugging for developers has gotten harder.
    When the behavior of an application depends on environmental factors 
    such as location, sensor inputs, and realistic cellular and wifi connectivity,
    it is important for developers to be able to understand how their app
    behaves out in the real world, not just at their desk.

    In order to enable the collection of remote execution data from real user's
    mobile devices, we propose the Live Remote Trace system (LRT). Our proposed
    approach uses instrumentation inserted into the Application Under Test at compile time,
    which then broadcasts events to a background listener service that is responsible for
    aggregating trace events and exporting the data to a cloud service of our
    own design. We address topics for enabling crowd-sourced trace data collection,
    and possible ways to mine the trace data to give developers actionable 
    information to improve their application.
\end{abstract}

\terms{Measurement}
\keywords{Mobile, Cloud, Debugging}

\section{Introduction}
One of the key tasks a mobile application developer must undertake is testing 
their application on real hardware, and with real users. This is important 
because there are many features that are available to apps running on hardware 
that are not available in emulation software. These include GPS, accelerometer, 
magnetometer, gyroscope, and Cellular data, as well as the rest of the 
complexity of the system that is impossible to recreate perfectly in an 
emulation layer. Real users and their environments also may exercise apps in
ways developers have not anticipated. 

We propose a system which will allow developers of applications on internet connected 
devices to easily embed lightweight instrumentation 
into their application code to enable high resolution 
tracing of program execution to a cloud service. Events are collected by 
the service which can perform analytics on program execution as it is 
exercised out in the wild, as well as from an aggregated 'big-data' view.

Our research plan consists of prototyping a vertical slice of the system.
This includes the server infrastructure which will recieve trace data,
a background service which runs on the mobile platform, and a sample
application which has instrumentation that can broadcast trace events to the background service.
In addition to investigating technologies that would make this technique 
feasible, we also will attempt to observe the processing and data workloads
involved, to better be able to select technologies used in a production version.

Successful completion of this work will serve to accomplish a few key 
improvements for testing mobile applications. This instrumentation will provide 
the developer with a trace of the program running on actual hardware as well as 
the opportunity to collect the trace while the application is 
subjected to real life user and environment interaction. With this 
body of program flow data collected on the server, app developers can see
whether the flow of their program aligns with their expectations, or if some strange
behavior was elicited during a trace. We also hypothesize that we can mine this data for useful 
information to answer questions such as ``\emph{which parts of the user interface were 
confusing to the user?}'', and ``\emph{Did they spend too much time wondering what to do 
next?}'', or ``\emph{which flows are common for users to take, and could thus be 
streamlined to create a better user experience?}'' Problems of this nature are 
discussed in \cite{WebAntiPattern}. 

In order for this work to be successful, we will need to 
prove out a few technical approaches. The first one will be to determine at what rate can we
export trace events from the application under test to the background LRT service 
before the user experience begins to degrade. 
We will also need to find on what order of magnitude is the data
that we are collecting, and how that will impact the export-to-server performance.
Finally, we will attempt to write some simple queries against our database of
trace events, to gain some initial statistics and data that would give a developer
actionable information about the behavior of their application.

The remainder of this paper is organized as follows. Up front, we will cover 
related work in section \ref{section:relatedwork}. In section \ref{section:challenges},
we will discuss design considerations that must be made when building our system, and in 
\ref{section:proposeddesign} we will address these challenges with an outline of
our proposed design. In section \ref{section:crowdsourcing} we will discuss how to drive
user participation for crowdsourcing, which is a critical non-technical 
challenge to make our approach successful. The techniques to mine data for useful 
information are outlined in \ref{section:datamining}
Finally, we will discuss how we plan to evaluate our work in \ref{section:evaluation}.

\input{relatedwork}
\input{challenges}
\input{proposeddesign}
\input{crowdsource}
\input{datamining}
\input{evaluation}
\input{conclusion}

\bibliography{citations}{}
\bibliographystyle{plain}

\end{document}

