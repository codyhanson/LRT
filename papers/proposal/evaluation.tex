\section{Empirical Evaluation}
\label{section:evaluation}
This section describes what we did to evaluate our proposal, and how well it performed
based on metrics we predefined.

\subsection{Plan}
There a few main things that will justify our research as an improvement on existing techniques.

First, it must objectively improve user experience, and do so better than other techniques.
To measure this, we will compare the speed with which a user navigates through a new app
before and after our UI improvements. To control this, we will have users use the two variants
of the app in both orders: improved version after original, and improved version before original.
Additionally, we will investigate options for simulating user input.

Second, it would be good to measure the time it takes developers to achieve equivalent
improvements in user experience with alternative techniques. This will be difficult to
measure objectively. Something we could do is use a public, popular app and start with
an old version, then compare the improvements our tool suggests to those the app team
implemented subsequent to that version. If our tool leads to a similar or better improvement
than the app team implemented, and does it in a reasonable time, this would indicate the
value of the LRT system.

Third, we must show that the LRT tool does not unreasonably impact the device's performance.
Since LRT aims to test UX and trace remotely, we cannot have the same expectations as testing
tools that store the results on the device. Additionally, we can run experiments to see how
much data, and how detailed of information we trace, before performance degradation exceeds
some limit. We propose that this threshold should be 20\% worse battery consumption, and 10\%
CPU and memory overhead.

\subsection{Setup}
For ease of implementation, we developed LRT for Android. We will use a Samsung Galaxy S3
over Wi-Fi of two signal strength levels (60dBm and 100dBm), a 3G connection, and a 4G connection
(Verizon). We will select an open source app that relies on listviews to run our tests on. We will
do some initial experimentation on our own to judge what level of detail our instrumentation can
trace without exceeding our performance impact thresholds. Based on this, we will insert some
amount of instrumentation into the app and have 5 randomly selected UCCS students use the app
in a consistent cellular service environment. Based on the results of LRT, we will modify the
app to improve user experience, after which we'll have another 5 randomly selected UCCS students
use the modified app. To judge the success of LRT, we will compare the performance metrics of the
two groups of students, as well as a reaction survey they will take after participating in the
study.

\subsection{Experiment Outcomes}
...


